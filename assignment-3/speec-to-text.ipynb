{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef5cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade datasets fsspec aiohttp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d254d",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c12056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 18:57:40.342209: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-04 18:57:40.354192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746365260.368956   38984 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746365260.374073   38984 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746365260.384183   38984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746365260.384198   38984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746365260.384200   38984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746365260.384205   38984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-04 18:57:40.388478: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import string "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5348ef",
   "metadata": {},
   "source": [
    "## Setting up variables for data downloading & training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bbf9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "login(token=\"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\")\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d84fc968",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10 \n",
    "validation_split = 0.2 \n",
    "\n",
    "max_samples = 1000\n",
    "max_audio_duration =5.0\n",
    "# ample_rate=16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f66175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"abcdefghijklmnopqrstuvwxyz '\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHARS = string.ascii_lowercase + \" '\"\n",
    "CHARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327bfb1",
   "metadata": {},
   "source": [
    "##  Preprocess functions : utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e950b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, sample_rate=16000, max_duration=5.0):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_duration = max_duration  # seconds\n",
    "        self.max_len_samples = int(sample_rate * max_duration)\n",
    "        self.max_len_time_steps = int(np.floor(self.max_len_samples / 512)) + 1\n",
    "\n",
    "\n",
    "    def load_and_process_audio(self, file_path):\n",
    "        \"\"\"Loads audio, pads/truncates, and computes log Mel spectrogram.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {file_path}: {e}\")\n",
    "            return None # Return None if loading fails\n",
    "\n",
    "        # Pad or truncate audio samples\n",
    "        if len(y) > self.max_len_samples:\n",
    "            y = y[:self.max_len_samples]\n",
    "        else:\n",
    "            y = np.pad(y, (0, max(0, self.max_len_samples - len(y))))\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        if log_mel_spec.shape[1] > self.max_len_time_steps:\n",
    "             log_mel_spec = log_mel_spec[:, :self.max_len_time_steps]\n",
    "        elif log_mel_spec.shape[1] < self.max_len_time_steps:\n",
    "             pad_width = self.max_len_time_steps - log_mel_spec.shape[1]\n",
    "             log_mel_spec = np.pad(log_mel_spec, ((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "       \n",
    "        return log_mel_spec.T\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.chars = sorted(list(CHARS))\n",
    "        self.char_to_int = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.padding_index = len(self.chars)\n",
    "        self.int_to_char = {i: c for c, i in self.char_to_int.items()}\n",
    "        self.int_to_char[self.padding_index] = \"<pad>\" \n",
    "\n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded = [self.char_to_int[c] for c in text.lower() if c in self.char_to_int]\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, seq):\n",
    "        decoded_chars = [self.int_to_char[i] for i in seq if i != self.padding_index]\n",
    "        return \"\".join(decoded_chars)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.chars) + 1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61021891",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80306703",
   "metadata": {},
   "source": [
    "Data used in training is fetched from hugging face \"common_voice_13_0\" , As it is taking high resource I have used the streaming to collect only 1000 samples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a51ef529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sample_rate=16000, max_samples=max_samples, max_audio_duration=max_audio_duration):\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        \"mozilla-foundation/common_voice_13_0\",\n",
    "        \"en\", \n",
    "        split=\"train\", \n",
    "        streaming=True,\n",
    "        storage_options={\"http\": {}} )\n",
    "\n",
    "   \n",
    "    print(f\"Taking the first {max_samples} samples...\")\n",
    "    dataset_head = list(islice(dataset, max_samples))\n",
    "\n",
    "    audio_proc = AudioPreprocessor(sample_rate, max_audio_duration)\n",
    "    text_proc = TextPreprocessor()\n",
    "\n",
    "    temp_audio_dir = \"temp_audio\"\n",
    "    os.makedirs(temp_audio_dir, exist_ok=True)\n",
    "\n",
    "    X = [] \n",
    "    texts = [] \n",
    "    processed_count = 0\n",
    "    for i, item in tqdm(enumerate(dataset_head), total=max_samples):\n",
    "        try:\n",
    "            audio_array = item[\"audio\"][\"array\"]\n",
    "          \n",
    "            path = os.path.join(temp_audio_dir, f\"sample_{i}.wav\")\n",
    "            sf.write(path, audio_array, samplerate=sample_rate)\n",
    "            processed_audio = audio_proc.load_and_process_audio(path)\n",
    "\n",
    "            original_text = item[\"sentence\"]\n",
    "\n",
    "            if processed_audio is not None:\n",
    "                X.append(processed_audio)\n",
    "                texts.append(original_text)\n",
    "                processed_count += 1\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {i}: {e}\")\n",
    "         \n",
    "\n",
    "    print(f\"Successfully processed {processed_count} samples.\")\n",
    "\n",
    "    print(\"Encoding text data...\")\n",
    "    encoded_texts = [text_proc.encode(t) for t in texts]\n",
    "    target_seq_length = audio_proc.max_len_time_steps\n",
    "    y = pad_sequences(\n",
    "        encoded_texts,\n",
    "        maxlen=target_seq_length, \n",
    "        padding='post', \n",
    "        value=text_proc.padding_index )\n",
    "\n",
    "    X = pad_sequences(X, padding='post', dtype='float32', value=0.0)\n",
    "    sample_weights = np.zeros_like(y, dtype=np.float32)\n",
    "    for i, seq in enumerate(y):\n",
    "        non_padding_indices = np.where(seq != text_proc.padding_index)[0]\n",
    "        if len(non_padding_indices) > 0:\n",
    "             sample_weights[i, non_padding_indices] = 1.0\n",
    "    min_samples = min(len(X), len(y), len(sample_weights))\n",
    "    X = np.array(X[:min_samples])\n",
    "    y = y[:min_samples]\n",
    "    sample_weights = sample_weights[:min_samples]\n",
    "\n",
    "    print(\"Data loading and preprocessing complete.\")\n",
    "    return X, y, sample_weights, text_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f13e845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking the first 1000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 1013968it [00:58, 17387.54it/s]\n",
      "100%|███████████████████████████████████████| 1000/1000 [00:22<00:00, 43.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 1000 samples.\n",
      "Encoding text data...\n",
      "Data loading and preprocessing complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1000, 157, 128), (1000, 157), (1000, 157))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, sample_weights, text_proc = load_data()\n",
    "\n",
    "X.shape , y.shape , sample_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c9a24",
   "metadata": {},
   "source": [
    "## Model : Bidirectional LSTM with time distributed layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3644c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def build_asr_model(input_shape, output_dim):\n",
    "#     inputs = layers.Input(shape=input_shape, name=\"input_features\")\n",
    "#     x = layers.Masking(mask_value=0.0, name=\"masking_input\")(inputs)\n",
    "#     x = layers.Bidirectional(layers.LSTM(128, return_sequences=True), name=\"bidirectional_lstm_1\")(x)\n",
    "#     x = layers.Bidirectional(layers.LSTM(128, return_sequences=True), name=\"bidirectional_lstm_2\")(x)\n",
    "#     x = layers.TimeDistributed(layers.Dense(256, activation='relu'), name=\"time_distributed_dense_1\")(x)\n",
    "#     outputs = layers.TimeDistributed(layers.Dense(output_dim, activation='softmax'), name=\"output_softmax\")(x)\n",
    "\n",
    "#     model = models.Model(inputs, outputs, name=\"asr_model\")\n",
    "#     model.compile(\n",
    "#         optimizer='adam', \n",
    "#         loss='sparse_categorical_crossentropy',\n",
    "#         metrics=['accuracy'] \n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "def build_asr_model(input_shape, output_dim, dropout_rate=0.2):\n",
    "    inputs = layers.Input(shape=input_shape, name=\"input_features\")\n",
    "    x = layers.Masking(mask_value=0.0, name=\"masking_input\")(inputs)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=dropout_rate), name=\"bidirectional_lstm_1\")(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=dropout_rate), name=\"bidirectional_lstm_2\")(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(256, activation='relu'), name=\"time_distributed_dense_1\")(x)\n",
    "    outputs = layers.TimeDistributed(layers.Dense(output_dim, activation='softmax'), name=\"output_softmax\")(x)\n",
    "    model = models.Model(inputs, outputs, name=\"asr_model\")\n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f55bb99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 19:00:41.213824: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "output_dim = text_proc.get_vocab_size()\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "# model = build_asr_model(input_shape, output_dim)\n",
    "model = build_asr_model(input_shape=(157, 128), output_dim=len(text_proc.chars)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c00b4c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"asr_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"asr_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_features      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ masking_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ any (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Any</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_lstm… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ masking_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_lstm… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ bidirectional_ls… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ bidirectional_ls… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_softmax      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,453</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_features      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_features[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ masking_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_features[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mMasking\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ any (\u001b[38;5;33mAny\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_lstm… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m263,168\u001b[0m │ masking_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_lstm… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m394,240\u001b[0m │ bidirectional_ls… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ bidirectional_ls… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_softmax      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m29\u001b[0m)   │      \u001b[38;5;34m7,453\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">730,653</span> (2.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m730,653\u001b[0m (2.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">730,653</span> (2.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m730,653\u001b[0m (2.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a268a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d90063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abbas/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n",
      "2025-05-04 19:00:49.853530: E tensorflow/core/util/util.cc:131] oneDNN supports DT_BOOL only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 5s/step - accuracy: 0.0468 - loss: 1.1241 - val_accuracy: 0.0575 - val_loss: 1.0972\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 9s/step - accuracy: 0.0581 - loss: 1.1062 - val_accuracy: 0.0596 - val_loss: 1.0963\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 6s/step - accuracy: 0.0585 - loss: 1.0792 - val_accuracy: 0.0596 - val_loss: 1.0933\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 4s/step - accuracy: 0.0598 - loss: 1.0995 - val_accuracy: 0.0612 - val_loss: 1.0921\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 4s/step - accuracy: 0.0599 - loss: 1.0829 - val_accuracy: 0.0618 - val_loss: 1.0906\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 4s/step - accuracy: 0.0605 - loss: 1.0746 - val_accuracy: 0.0613 - val_loss: 1.0901\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 4s/step - accuracy: 0.0605 - loss: 1.0798 - val_accuracy: 0.0625 - val_loss: 1.0898\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 4s/step - accuracy: 0.0615 - loss: 1.0900 - val_accuracy: 0.0612 - val_loss: 1.0897\n",
      "Epoch 9/10\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=validation_split,\n",
    "        sample_weight=sample_weights\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2353ad",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c3a716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "ModelPath = \"Model/asr_model.h5\" \n",
    "model.save(ModelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a16f2e",
   "metadata": {},
   "source": [
    "## Loading Model for Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec88f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(ModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6478e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_long_audio(model, audio_path, sample_rate=16000, chunk_duration=5.0, overlap=1.0):\n",
    "    audio_proc = AudioPreprocessor(sample_rate, chunk_duration)\n",
    "    text_proc = TextPreprocessor()\n",
    "\n",
    "    y, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "    chunk_samples = int(sample_rate * chunk_duration)\n",
    "    step_size = int(chunk_samples - sample_rate * overlap)\n",
    "\n",
    "    transcript = \"\"\n",
    "\n",
    "    for start in range(0, len(y), step_size):\n",
    "        end = start + chunk_samples\n",
    "        chunk = y[start:end]\n",
    "\n",
    "        if len(chunk) < chunk_samples:\n",
    "            chunk = np.pad(chunk, (0, chunk_samples - len(chunk)), mode='constant')\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=128)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        if log_mel_spec.shape[1] > 157:\n",
    "            log_mel_spec = log_mel_spec[:, :157]\n",
    "        elif log_mel_spec.shape[1] < 157:\n",
    "            pad_width = 157 - log_mel_spec.shape[1]\n",
    "            log_mel_spec = np.pad(log_mel_spec, ((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "        input_data = log_mel_spec.T[np.newaxis, ...] \n",
    "        prediction = model.predict(input_data)\n",
    "        predicted_ids = np.argmax(prediction, axis=-1)[0]\n",
    "        decoded_text = text_proc.decode(predicted_ids)\n",
    "        transcript += decoded_text.strip() + \" \"\n",
    "\n",
    "    return transcript.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63b279f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the the'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audioPath = \"audio2.mpga\"\n",
    "\n",
    "transcribe_long_audio(model , audioPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2fe6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
