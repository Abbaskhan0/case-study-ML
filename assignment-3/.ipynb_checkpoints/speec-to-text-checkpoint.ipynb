{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef5cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade datasets fsspec aiohttp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d254d",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c12056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 17:51:56.916832: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-04 17:51:56.999332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746361317.065639   12522 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746361317.077862   12522 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746361317.151852   12522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746361317.151878   12522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746361317.151880   12522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746361317.151882   12522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-04 17:51:57.162734: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import string "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5348ef",
   "metadata": {},
   "source": [
    "## Setting up variables for data downloading & training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bbf9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "login(token=\"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\")\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fc968",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 1000\n",
    "max_audio_duration =5.0\n",
    "# ample_rate=16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f66175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"abcdefghijklmnopqrstuvwxyz '\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHARS = string.ascii_lowercase + \" '\"\n",
    "CHARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61021891",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80306703",
   "metadata": {},
   "source": [
    "### Data used in training is fetched from hugging face \"common_voice_13_0\" , As it is taking high resource I have used the streaming to collect only 1000 samples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ef529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sample_rate=16000, max_samples=max_samples, max_audio_duration=max_audio_duration):\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        \"mozilla-foundation/common_voice_13_0\",\n",
    "        \"en\", \n",
    "        split=\"train\", \n",
    "        streaming=True,\n",
    "        storage_options={\"http\": {}} )\n",
    "\n",
    "   \n",
    "    print(f\"Taking the first {max_samples} samples...\")\n",
    "    dataset_head = list(islice(dataset, max_samples))\n",
    "\n",
    "    audio_proc = AudioPreprocessor(sample_rate, max_audio_duration)\n",
    "    text_proc = TextPreprocessor()\n",
    "\n",
    "    temp_audio_dir = \"temp_audio\"\n",
    "    os.makedirs(temp_audio_dir, exist_ok=True)\n",
    "\n",
    "    X = [] \n",
    "    texts = [] \n",
    "    processed_count = 0\n",
    "    for i, item in tqdm(enumerate(dataset_head), total=max_samples):\n",
    "        try:\n",
    "            audio_array = item[\"audio\"][\"array\"]\n",
    "          \n",
    "            path = os.path.join(temp_audio_dir, f\"sample_{i}.wav\")\n",
    "            sf.write(path, audio_array, samplerate=sample_rate)\n",
    "            processed_audio = audio_proc.load_and_process_audio(path)\n",
    "\n",
    "            original_text = item[\"sentence\"]\n",
    "\n",
    "            if processed_audio is not None:\n",
    "                X.append(processed_audio)\n",
    "                texts.append(original_text)\n",
    "                processed_count += 1\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {i}: {e}\")\n",
    "         \n",
    "\n",
    "    print(f\"Successfully processed {processed_count} samples.\")\n",
    "\n",
    "    print(\"Encoding text data...\")\n",
    "    encoded_texts = [text_proc.encode(t) for t in texts]\n",
    "    target_seq_length = audio_proc.max_len_time_steps\n",
    "    y = pad_sequences(\n",
    "        encoded_texts,\n",
    "        maxlen=target_seq_length, \n",
    "        padding='post', \n",
    "        value=text_proc.padding_index )\n",
    "\n",
    "    # Pad X to ensure consistent shape across all samples (should be consistent if audio processing is correct)\n",
    "    # Use a padding value that the Masking layer in the model will ignore (0.0 in your model)\n",
    "    X = pad_sequences(X, padding='post', dtype='float32', value=0.0)\n",
    "\n",
    "    # Create sample weights: 1 for actual characters, 0 for padding\n",
    "    # This mask tells the loss function which time steps to consider\n",
    "    sample_weights = np.zeros_like(y, dtype=np.float32)\n",
    "    for i, seq in enumerate(y):\n",
    "        # Find where the actual data ends (before padding starts)\n",
    "        # np.where returns a tuple, we need the first element (the array of indices)\n",
    "        non_padding_indices = np.where(seq != text_proc.padding_index)[0]\n",
    "        if len(non_padding_indices) > 0:\n",
    "             # The weight should be 1.0 for all non-padding indices\n",
    "             sample_weights[i, non_padding_indices] = 1.0\n",
    "\n",
    "\n",
    "    # Ensure X, y, and sample_weights have the exact same number of samples\n",
    "    # This handles cases where some samples might have failed processing\n",
    "    min_samples = min(len(X), len(y), len(sample_weights))\n",
    "    X = np.array(X[:min_samples])\n",
    "    y = y[:min_samples]\n",
    "    sample_weights = sample_weights[:min_samples]\n",
    "\n",
    "    print(\"Data loading and preprocessing complete.\")\n",
    "    return X, y, sample_weights, text_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3a716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ASR model training script...\n",
      "Loading Common Voice dataset...\n",
      "Taking the first 5000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 1013968it [00:59, 16928.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio and text data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [02:23<00:00, 34.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 5000 samples.\n",
      "Encoding text data...\n",
      "Target sequence length for text (matching audio time steps): 157\n",
      "Data loading and preprocessing complete.\n",
      "\n",
      "Data shapes after loading and preprocessing:\n",
      "Shape of X (audio features): (5000, 157, 128)\n",
      "Shape of y (target text sequences): (5000, 157)\n",
      "Shape of sample_weights (mask for loss): (5000, 157)\n",
      "Output dimension (vocabulary size including padding): 29\n",
      "\n",
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 17:29:54.172730: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"asr_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"asr_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_features      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ masking_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ any (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Any</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_lstm… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ masking_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_lstm… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ bidirectional_ls… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ bidirectional_ls… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_softmax      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,453</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_features      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_features[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ masking_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_features[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mMasking\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ any (\u001b[38;5;33mAny\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_lstm… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m263,168\u001b[0m │ masking_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_lstm… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m394,240\u001b[0m │ bidirectional_ls… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ bidirectional_ls… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_softmax      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m29\u001b[0m)   │      \u001b[38;5;34m7,453\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">730,653</span> (2.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m730,653\u001b[0m (2.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">730,653</span> (2.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m730,653\u001b[0m (2.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abbas/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 17:29:55.297937: E tensorflow/core/util/util.cc:131] oneDNN supports DT_BOOL only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m593s\u001b[0m 5s/step - accuracy: 0.0569 - loss: 1.1037 - val_accuracy: 0.0601 - val_loss: 1.0841\n",
      "Epoch 2/10\n",
      "\u001b[1m 42/125\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:26\u001b[0m 4s/step - accuracy: 0.0612 - loss: 1.0880"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, sample_rate=16000, max_duration=5.0):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_duration = max_duration  # seconds\n",
    "        self.max_len_samples = int(sample_rate * max_duration)\n",
    "        # Calculate the exact number of time steps after Mel spectrogram\n",
    "        # Using default n_fft=2048, hop_length=512.\n",
    "        # The formula is floor(n_samples / hop_length) + 1 for center=True (default)\n",
    "        self.max_len_time_steps = int(np.floor(self.max_len_samples / 512)) + 1\n",
    "\n",
    "\n",
    "    def load_and_process_audio(self, file_path):\n",
    "        \"\"\"Loads audio, pads/truncates, and computes log Mel spectrogram.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {file_path}: {e}\")\n",
    "            return None # Return None if loading fails\n",
    "\n",
    "        # Pad or truncate audio samples\n",
    "        if len(y) > self.max_len_samples:\n",
    "            y = y[:self.max_len_samples]\n",
    "        else:\n",
    "            y = np.pad(y, (0, max(0, self.max_len_samples - len(y))))\n",
    "\n",
    "        # Compute Mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "        # Convert to log scale\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        # Ensure the time dimension is exactly max_len_time_steps\n",
    "        # This handles potential off-by-one issues with librosa padding/truncation\n",
    "        if log_mel_spec.shape[1] > self.max_len_time_steps:\n",
    "             log_mel_spec = log_mel_spec[:, :self.max_len_time_steps]\n",
    "        elif log_mel_spec.shape[1] < self.max_len_time_steps:\n",
    "             # Pad the time dimension if necessary (shouldn't happen with fixed audio length, but as a safeguard)\n",
    "             pad_width = self.max_len_time_steps - log_mel_spec.shape[1]\n",
    "             log_mel_spec = np.pad(log_mel_spec, ((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "        # Transpose to (Time, Features) for the model input\n",
    "        return log_mel_spec.T\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        # Use the predefined character set and add a padding character\n",
    "        # Sort to ensure consistent mapping\n",
    "        self.chars = sorted(list(CHARS))\n",
    "        # Create mapping from character to integer ID\n",
    "        self.char_to_int = {c: i for i, c in enumerate(self.chars)}\n",
    "        # Reserve the last index for padding\n",
    "        self.padding_index = len(self.chars)\n",
    "        # Create mapping from integer ID back to character/token\n",
    "        self.int_to_char = {i: c for c, i in self.char_to_int.items()}\n",
    "        self.int_to_char[self.padding_index] = \"<pad>\" # Define a token string for padding\n",
    "\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encodes text to a sequence of integers.\"\"\"\n",
    "        # Convert text to lowercase and filter out characters not in our vocabulary\n",
    "        encoded = [self.char_to_int[c] for c in text.lower() if c in self.char_to_int]\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, seq):\n",
    "        \"\"\"Decodes a sequence of integers back to text.\"\"\"\n",
    "        # Decode, ignoring the padding index\n",
    "        decoded_chars = [self.int_to_char[i] for i in seq if i != self.padding_index]\n",
    "        return \"\".join(decoded_chars)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        \"\"\"Returns the size of the vocabulary, including the padding token.\"\"\"\n",
    "        return len(self.chars) + 1 # +1 for the padding index\n",
    "\n",
    "\n",
    "def load_data(sample_rate=16000, max_samples=1000, max_audio_duration=5.0):\n",
    "    \"\"\"Loads and preprocesses audio and text data from Common Voice.\"\"\"\n",
    "    print(\"Loading Common Voice dataset...\")\n",
    "    dataset = load_dataset(\n",
    "        \"mozilla-foundation/common_voice_13_0\",\n",
    "        \"en\", # Specify the English subset\n",
    "        split=\"train\", # Use the training split\n",
    "        streaming=True, # Stream the data to avoid loading everything into memory\n",
    "        storage_options={\"http\": {}} # Required for streaming from http\n",
    "    )\n",
    "\n",
    "    # Grab first max_samples from the streaming dataset\n",
    "    print(f\"Taking the first {max_samples} samples...\")\n",
    "    dataset_head = list(islice(dataset, max_samples))\n",
    "\n",
    "    audio_proc = AudioPreprocessor(sample_rate, max_audio_duration)\n",
    "    text_proc = TextPreprocessor()\n",
    "\n",
    "    # Create a temporary directory to save audio files\n",
    "    temp_audio_dir = \"temp_audio\"\n",
    "    os.makedirs(temp_audio_dir, exist_ok=True)\n",
    "\n",
    "    X = [] # List to store processed audio features\n",
    "    texts = [] # List to store original text transcripts\n",
    "    print(\"Processing audio and text data...\")\n",
    "    processed_count = 0\n",
    "    for i, item in tqdm(enumerate(dataset_head), total=max_samples):\n",
    "        try:\n",
    "            audio_array = item[\"audio\"][\"array\"]\n",
    "            # Create a temporary file path\n",
    "            path = os.path.join(temp_audio_dir, f\"sample_{i}.wav\")\n",
    "            # Save the audio array to a temporary WAV file\n",
    "            sf.write(path, audio_array, samplerate=sample_rate)\n",
    "\n",
    "            # Process the audio file\n",
    "            processed_audio = audio_proc.load_and_process_audio(path)\n",
    "\n",
    "            # Process the text transcript\n",
    "            original_text = item[\"sentence\"]\n",
    "\n",
    "            # Only append if audio processing was successful\n",
    "            if processed_audio is not None:\n",
    "                X.append(processed_audio)\n",
    "                texts.append(original_text)\n",
    "                processed_count += 1\n",
    "            else:\n",
    "                print(f\"Skipping sample {i} due to audio processing error.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {i}: {e}\")\n",
    "            # Continue to the next sample if an error occurs\n",
    "\n",
    "    print(f\"Successfully processed {processed_count} samples.\")\n",
    "\n",
    "    # Encode texts to integer sequences using the TextPreprocessor\n",
    "    print(\"Encoding text data...\")\n",
    "    encoded_texts = [text_proc.encode(t) for t in texts]\n",
    "\n",
    "    # Determine the target sequence length based on the audio time steps\n",
    "    target_seq_length = audio_proc.max_len_time_steps\n",
    "    print(f\"Target sequence length for text (matching audio time steps): {target_seq_length}\")\n",
    "\n",
    "    # Pad the encoded text sequences to the target sequence length (157)\n",
    "    # Use the padding_index defined in TextPreprocessor\n",
    "    y = pad_sequences(\n",
    "        encoded_texts,\n",
    "        maxlen=target_seq_length, # Pad to the same length as audio time steps\n",
    "        padding='post', # Pad at the end\n",
    "        value=text_proc.padding_index # Use the padding index for padding\n",
    "    )\n",
    "\n",
    "    # Pad X to ensure consistent shape across all samples (should be consistent if audio processing is correct)\n",
    "    # Use a padding value that the Masking layer in the model will ignore (0.0 in your model)\n",
    "    X = pad_sequences(X, padding='post', dtype='float32', value=0.0)\n",
    "\n",
    "    # Create sample weights: 1 for actual characters, 0 for padding\n",
    "    # This mask tells the loss function which time steps to consider\n",
    "    sample_weights = np.zeros_like(y, dtype=np.float32)\n",
    "    for i, seq in enumerate(y):\n",
    "        # Find where the actual data ends (before padding starts)\n",
    "        # np.where returns a tuple, we need the first element (the array of indices)\n",
    "        non_padding_indices = np.where(seq != text_proc.padding_index)[0]\n",
    "        if len(non_padding_indices) > 0:\n",
    "             # The weight should be 1.0 for all non-padding indices\n",
    "             sample_weights[i, non_padding_indices] = 1.0\n",
    "\n",
    "\n",
    "    # Ensure X, y, and sample_weights have the exact same number of samples\n",
    "    # This handles cases where some samples might have failed processing\n",
    "    min_samples = min(len(X), len(y), len(sample_weights))\n",
    "    X = np.array(X[:min_samples])\n",
    "    y = y[:min_samples]\n",
    "    sample_weights = sample_weights[:min_samples]\n",
    "\n",
    "    print(\"Data loading and preprocessing complete.\")\n",
    "    return X, y, sample_weights, text_proc\n",
    "\n",
    "def build_asr_model(input_shape, output_dim):\n",
    "    \"\"\"Builds the ASR model using Bidirectional LSTMs and TimeDistributed Dense layers.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape, name=\"input_features\")\n",
    "\n",
    "    # Masking layer to ignore padded audio frames (value=0.0 as used in pad_sequences for X)\n",
    "    # This layer automatically generates a mask based on the mask_value\n",
    "    # The mask is then propagated to subsequent layers that support masking\n",
    "    x = layers.Masking(mask_value=0.0, name=\"masking_input\")(inputs)\n",
    "\n",
    "    # Bidirectional LSTM layers process the sequence in both forward and backward directions\n",
    "    # They support masking and propagate the mask\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True), name=\"bidirectional_lstm_1\")(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True), name=\"bidirectional_lstm_2\")(x)\n",
    "\n",
    "    # TimeDistributed Dense layers apply a Dense layer independently to each time step\n",
    "    # They are designed to handle masks propagated from previous layers\n",
    "    x = layers.TimeDistributed(layers.Dense(256, activation='relu'), name=\"time_distributed_dense_1\")(x)\n",
    "    # Output layer predicts probability distribution over characters for each time step\n",
    "    # The output dimension is the size of the vocabulary (including padding)\n",
    "    outputs = layers.TimeDistributed(layers.Dense(output_dim, activation='softmax'), name=\"output_softmax\")(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs, name=\"asr_model\")\n",
    "    model.compile(\n",
    "        optimizer='adam', # Adam optimizer is a good default\n",
    "        loss='sparse_categorical_crossentropy', # Appropriate loss for integer targets and softmax output\n",
    "        metrics=['accuracy'] # Monitor accuracy during training\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting ASR model training script...\")\n",
    "    # Adjust max_samples for faster testing if needed\n",
    "    # A larger number of samples will give a more meaningful result but take longer\n",
    "    MAX_SAMPLES_TO_LOAD = 5000 # Keep small for quick testing\n",
    "\n",
    "    X, y, sample_weights, text_proc = load_data(max_samples=MAX_SAMPLES_TO_LOAD)\n",
    "\n",
    "    print(\"\\nData shapes after loading and preprocessing:\")\n",
    "    print(f\"Shape of X (audio features): {X.shape}\")\n",
    "    print(f\"Shape of y (target text sequences): {y.shape}\")\n",
    "    print(f\"Shape of sample_weights (mask for loss): {sample_weights.shape}\")\n",
    "\n",
    "    # Get the vocabulary size including the padding token\n",
    "    output_dim = text_proc.get_vocab_size()\n",
    "    print(f\"Output dimension (vocabulary size including padding): {output_dim}\")\n",
    "\n",
    "    print(\"\\nBuilding model...\")\n",
    "    # Input shape for the model is (time_steps, features) excluding the batch size\n",
    "    input_shape = (X.shape[1], X.shape[2])\n",
    "    model = build_asr_model(input_shape, output_dim)\n",
    "\n",
    "    # Print the model summary to see the layers and parameter counts\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\nTraining model...\")\n",
    "    batch_size = 32 # Number of samples per gradient update\n",
    "    epochs = 10 # Number of passes over the entire dataset\n",
    "    validation_split = 0.2 # Fraction of the training data to use for validation\n",
    "\n",
    "    # Train the model\n",
    "    # Pass sample_weight to ignore the loss from padded target steps\n",
    "    history = model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=validation_split,\n",
    "        sample_weight=sample_weights # Pass the sample weights here to mask loss\n",
    "    )\n",
    "\n",
    "    print(\"\\nTraining finished.\")\n",
    "\n",
    "    # Optional: Save the trained model\n",
    "    # try:\n",
    "    #     model.save(\"asr_model.h5\")\n",
    "    #     print(\"Model saved to asr_model.h5\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error saving model: {e}\")\n",
    "\n",
    "    # Optional: You might want to save the text_proc object or its mappings\n",
    "    # so you can use it for inference later to decode model predictions.\n",
    "    # For example, save char_to_int and int_to_char to a JSON file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d43e2525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"abcdefghijklmnopqrstuvwxyz '\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88f667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
